{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84334110",
   "metadata": {},
   "source": [
    "## Object Detection + VLMs(Visual Language Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeef656",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733de9c3",
   "metadata": {},
   "source": [
    "⸻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ad46c",
   "metadata": {},
   "source": [
    "**Problems &  Motivation**\n",
    "1. Closed-set | 모델을 사전에 정의 + 학습된 제한된 객체만 인식 가능\n",
    "\n",
    "2. Retraining | 새로운 객체 or 새로운 환경에 모델을 적용하는 경우 <br> dataset rebuild, retraining, finetuning 필요\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "**New approch**\n",
    "\n",
    "<br> object detection + agentic - 시간↓ 비용↓ 정확도↑\n",
    "\n",
    "1. **Open-Vocabulary**\n",
    "<br>\n",
    "    * Multi modal architecture | Transformer 기반 CLIP, 텍스트와 이미지의 공유 임베딩 공간 학습<br>\n",
    "    훈련 데이터에 없던 새로운 클래나 문구를 이해 + 이미지의 시각적 특징과 연결<br>\n",
    "    GroundingDINO, OWL-ViT 와 같은 모델 사용\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Agentic Framework**\n",
    "<br>\n",
    "\n",
    "    * CLIP 기반 object detection process(GroundingDINO, OWL-ViT ) +  LLM(GPT, Gemini etc.. )\n",
    "    * **한 번의 추론 과정** 내에서 검증과 비평 과정 수행\n",
    "    * 깊게 생각하게 만들어 재학습 없이도 정확도 ↑\n",
    "    * 교차 검증 방식을 상황/ 조건에 맞게 **선택, 조합, 반복 그리고 중지**할 수 있는 구조  \n",
    "    → Agentic 성격 부여\n",
    "    * 사용자의 의도에 부합하는 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb919fa",
   "metadata": {},
   "source": [
    "<img src=\"../notebook/figure/workflow_image.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6603ac",
   "metadata": {},
   "source": [
    "**VLM 호출 최적화**\n",
    "<br> 검증의 방식으로 가능한 경우의 수\n",
    "\n",
    "1. 개별 검증 | Brute-force crop loop \n",
    "    * 직관적인 방법 \n",
    "    * 생성된 모든 바운딩 박스를 각각 독립적으로 처리<br>즉, n개의 bbox = n번의 처리\n",
    "    * 바운딩 박스 영역만 잘라내기<br>→ base64 형식으로 인코딩<br>→ VLM에 질문할 프롬프트 생성 <br>→VLM API 분석<br>→VLM의 답변에 따라 제거 또는 유지\n",
    "    * 시간↑ 비용↑\n",
    "\n",
    "2. Numbered Batching \n",
    "    * Numbered-arrow annotation\n",
    "    * 모든 detection 결과에 번호와 화살표를 단 이미지 한 장을 VLM으로 전달\n",
    "    * 시간↓ 비용↓ 정확도↑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d7182",
   "metadata": {},
   "source": [
    "**High Level Design**\n",
    "1. **Concept Detector**\n",
    "    <br>image + user request <br>→ VLM(a)이 이미지와 사용자 요청을 분석해 detection할 객체의 개념 인식 +  object labels 추출\n",
    "\n",
    "2. **Initial Object Detection**\n",
    "    <br>image + object labels <br>→ Open-Vocabulary Object Detector를 사용, 이미지 내 bbox 생성\n",
    "\n",
    "3. Visualization\n",
    "    <br>image + initial detection <br>→ 탐지한 객체를 원본 이미지에 화살표, 숫자 라벨로 표시한 annotated image 생성\n",
    "\n",
    "4. Query Critique & Refinement\n",
    "    <br>annotated image + user request <br>→ VLM(b) 이 초기 탐지 결과 검토, <br>탐지 결과가 사용자의 의도와 맞지 않거나 부적절한 경우 더 일반적이거나 적합한 개념으로 object label 수정\n",
    "\n",
    "5. Refined Detection\n",
    "    <br>image + 개선한 object labels <br>→ Open-Vocabulary Object Detector를 사용, 이미지 내 bbox 생성\n",
    "\n",
    "6. Final Filtering\n",
    "    <br>image + user request + detection <br>→ VLM(a) 교차 검증<br>개선된 탐지 결과가 사용자의 의도와 일치하는지 검증, 사용자의 의도와 불일치 or 무관한 객체 제거  \n",
    "\n",
    "7. Outut packaging\n",
    "    <br>최종 검증된 bbox가 표시된 이미지\n",
    "    <br>모든 검증 단계를 통과해 사용자 요청과 가장 일치하는 객체만 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0957acb",
   "metadata": {},
   "source": [
    "<img src=\"../notebook/figure/workflow.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50c2d6",
   "metadata": {},
   "source": [
    "<img src = \"../notebook/figure/workflow_image.jpg\" width = \"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2467025",
   "metadata": {},
   "source": [
    "<ime src=\"notebook\\figure\\workflow_image.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee31f37",
   "metadata": {},
   "source": [
    "**설계 계획**\n",
    "\n",
    "\n",
    "1. VLM | VLM과의 연결\n",
    "    * 프롬프트 구성, API 호출, 응답 파싱 \n",
    "    * 메서드\n",
    "        * init: OpenAI 클라이언트 초기화  \n",
    "        * chat completion: VLM API 요청, 응답 저장\n",
    "        * extract objects bfrom request: 탐지 객체 목록 문자열 리스트로 추출\n",
    "\n",
    "2. Detecion | agentic pipeline 전체\n",
    "    * 메서드\n",
    "        * init: detection model processor 로드, VLM 인스턴스 주입\n",
    "        * run: 전체 파이프라인 실행, 최종 결과 이미지 반환 \n",
    "    * 내부 메서드        \n",
    "        * critique, refine: 사용자의 요청에 부합하는지 비평, 개선된 객체 목록 추출\n",
    "        * validate bbox num: 번호가 매겨진 탐지 이미지에서 사용자 의도와 일치하는 객체 번호 목록 추출\n",
    "        * run detector: detetion 모델을 직접 실행해 이미지와 객체 리스에서 바운딩 박스 좌표 추출\n",
    "    \n",
    "\n",
    "3. Utils | 반복 사용 함수\n",
    "    * image encoding\n",
    "    * draw arrow and numberin\n",
    "    * draw bounding box \n",
    "\n",
    "4. Parameter configuration | 기본 설정\n",
    "    * model\n",
    "        * detector model type: hugging face ID\n",
    "        * initial VLM: 초기 개념 추출에 사용할 VLM 모델 이름\n",
    "        * critique VLM: 쿼리 비평 및 개선에 사용할 VLM 모델 이름\n",
    "        * validation VLM: 최종 검증에 사용할 VLM 모델 이름\n",
    "    * execution configuration \n",
    "        * 코드 실행 장치: cuda or cpu\n",
    "        * confidence threshold\n",
    "    * visualization configuration: detection의 신뢰도\n",
    "        * color palette: 바운딩 박스 및 라벨 시각화 \n",
    "        * font path: 라벨 텍스트 그릴 때 사용할 폰트 파일 경로"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
