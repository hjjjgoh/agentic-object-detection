{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0201bc27",
   "metadata": {},
   "source": [
    "**[Pipeline Implementation]**\n",
    "\n",
    "1. VLM Tool\n",
    "2. Agentic object detection pipeline\n",
    "3. Running the object detector\n",
    "4. Why Numbered Batching for Inferencing\n",
    "5. Critiquing and Refining the Query\n",
    "6. Validation Bounding Box predictions utilizing the VLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1adb97f",
   "metadata": {},
   "source": [
    "### 1. VLM Tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b34197",
   "metadata": {},
   "source": [
    "* Input\n",
    "    * Image(path/ base64)\n",
    "    * User request\n",
    "* Output\n",
    "    * Object: [str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ad5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function\n",
    "# 프로젝트 루트 탐색\n",
    "def find_project_root(marker_filename=\".project-root\"):\n",
    "    current_dir = os.path.abspath(os.getcwd())\n",
    "    while True:\n",
    "        if os.path.isfile(os.path.join(current_dir, marker_filename)):\n",
    "            return current_dir\n",
    "        parent_dir = os.path.dirname(current_dir)\n",
    "        if parent_dir == current_dir:\n",
    "            raise FileNotFoundError(f\"Could not find {marker_filename} in any parent directory.\")\n",
    "        current_dir = parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f86246cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9f465b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# from utils.image_utils import encode_image\n",
    "\n",
    "class VLMTool:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        messages,\n",
    "        model=\"o1\",\n",
    "        max_tokens=300,\n",
    "        temperature=0.1,\n",
    "        response_format=None\n",
    "    ):\n",
    "        \"\"\"Calls GPT for chat completion.\"\"\"\n",
    "        try:\n",
    "            if model in [\"gpt-4o\", \"gpt-4o-mini\"]:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    response_format=response_format if response_format else {\"type\": \"text\"}\n",
    "                )\n",
    "            elif model in [\"o1\"]:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    response_format=response_format if response_format else {\"type\": \"text\"}\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(\"This model is not supported\")\n",
    "\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling LLM: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_objects_from_request(self, image_path, user_text, model=\"gpt-4o\"):\n",
    "        \"\"\" Asks the LLM to parse user request for which objects to detect/segment.\n",
    "        Returns a list of objects in plain text.\"\"\"\n",
    "        base64_image = encode_image(image_path)\n",
    "        if not base64_image:\n",
    "            return None\n",
    "\n",
    "        prompt = (\n",
    "            \"You are an AI vision assistant that extracts objects to be identified from a user's request.\"\n",
    "            \"If the user wants to detect or semantically segment all objects in the image, return a comma-separated list of objects you can see. \"\n",
    "            \"If the user wants to detect or semantically segment specific objects, extract only those mentioned explicitly in their request. \"\n",
    "            \"Respond ONLY with the list of objects, separated by commas, and NOTHING ELSE.\"\n",
    "            \"The objective here is only to understand the objects of interest that can be extracted from the image and the user's request.\"\n",
    "            \"You are not actually required to perform or execute the user's request.\"    \n",
    "            )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": user_text},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            \"detail\": \"high\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        result = self.chat_completion(messages, model=model)\n",
    "        if result:\n",
    "            detected_objects = [\n",
    "                obj.strip().lower()\n",
    "                for obj in result.split(\",\")\n",
    "                if obj.strip()\n",
    "            ]\n",
    "            return detected_objects\n",
    "\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fbfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the velvet cloak of night so deep,  \n",
      "A solitary star begins to weep.  \n",
      "It hangs alone in the vast expanse,  \n",
      "Yearning for a kindred glance.  \n",
      "\n",
      "Surrounded by an endless void,  \n",
      "Its silent glow is unalloyed.  \n",
      "A shimmering tear in the cosmic seam,  \n",
      "A beacon lost in an endless dream.  \n",
      "\n",
      "Oh, lonely star in the sky's embrace,  \n",
      "Does solitude enhance your grace?  \n",
      "Or do you long for a companion's light,  \n",
      "To share the burden of the night?  \n",
      "\n",
      "Yet in your solitude, you shine,  \n",
      "A gem of hope in the dark divine.  \n",
      "Though you wander the heavens afar,  \n",
      "You teach us the strength of a lonely star.  \n"
     ]
    }
   ],
   "source": [
    "# chat_completion() 출력 예시 \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_root = find_project_root()\n",
    "dotenv_path = os.path.join(project_root, \".env\")\n",
    "load_dotenv(dotenv_path) # .env 파일의 환경 변수를 os.environ에 로드 / 환경변수 전부 로드\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = VLMTool(api_key=api_key)\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a creative poet.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short poem about a lonely star.\"}\n",
    "]\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=chat_messages,\n",
    "    model=\"gpt-4o\",\n",
    "    max_tokens=150,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_objects_from_request() 출력 예시\n",
    "# extract_objects_from_request() 출력 예시 \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_root = find_project_root()\n",
    "dotenv_path = os.path.join(project_root, \".env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = VLMTool(api_key=api_key)\n",
    "\n",
    "# 테스트할 이미지 경로 (실제 이미지 경로로 변경하세요)\n",
    "image_path = r\"C:\\Users\\KIST\\agentic-object-detection\\data\\image1.jpg\"\n",
    "\n",
    "\n",
    "# 테스트 케이스 1: 모든 객체 탐지\n",
    "user_request_1 = \"이 이미지에 있는 모든 객체를 찾아줘\"\n",
    "objects_1 = client.extract_objects_from_request(image_path, user_request_1)\n",
    "print(f\"요청: {user_request_1}\")\n",
    "print(f\"결과: {objects_1}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f5816",
   "metadata": {},
   "source": [
    "### 2. Agentic object detection pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603da86",
   "metadata": {},
   "source": [
    "### 3. Running the object detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a976f8",
   "metadata": {},
   "source": [
    "### 4. Why Numbered Batching for Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b413256",
   "metadata": {},
   "source": [
    "### 5. Critiquing and Refining the Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809ae88",
   "metadata": {},
   "source": [
    "### 6. Validation Bounding Box predictions utilizing the VLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
