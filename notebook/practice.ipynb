{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0201bc27",
   "metadata": {},
   "source": [
    "**[Pipeline Implementation]**\n",
    "\n",
    "1. VLM Tool\n",
    "2. Agentic object detection pipeline\n",
    "3. Running the object detector\n",
    "4. Why Numbered Batching for Inferencing\n",
    "5. Critiquing and Refining the Query\n",
    "6. Validation Bounding Box predictions utilizing the VLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1adb97f",
   "metadata": {},
   "source": [
    "### 1. VLM Tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86246cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f465b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# from utils.image_utils import encode_image\n",
    "\n",
    "class VLMTool:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        messages,\n",
    "        model=\"o1\",\n",
    "        max_tokens=300,\n",
    "        temperature=0.1,\n",
    "        response_format=None\n",
    "    ):\n",
    "        \"\"\"Calls GPT for chat completion.\"\"\"\n",
    "        try:\n",
    "            if model in [\"gpt-4o\", \"gpt-4o-mini\"]:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    response_format=response_format if response_format else {\"type\": \"text\"}\n",
    "                )\n",
    "            elif model in [\"o1\"]:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    response_format=response_format if response_format else {\"type\": \"text\"}\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(\"This model is not supported\")\n",
    "\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling LLM: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_objects_from_request(self, image_path, user_text, model=\"gpt-4o\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fbfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the vast velvet sky so wide,  \n",
      "A lonely star takes its nightly stride.  \n",
      "With a shimmer soft, it bravely gleams,  \n",
      "Whispering secrets through cosmic dreams.  \n",
      "\n",
      "No constellations to call its kin,  \n",
      "Yet in solitude, its light begins.  \n",
      "A beacon bright in night's embrace,  \n",
      "Graceful in its silent, solitary space.  \n",
      "\n",
      "It watches worlds with a gentle sigh,  \n",
      "Dreaming of friends in the endless sky.  \n",
      "Though alone, it never fades away,  \n",
      "Guiding lost souls to a brand new day.  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Find project root using .project-root marker file\n",
    "def find_project_root(marker_filename=\".project-root\"):\n",
    "    current_dir = os.path.abspath(os.getcwd())\n",
    "    while True:\n",
    "        if os.path.isfile(os.path.join(current_dir, marker_filename)):\n",
    "            return current_dir\n",
    "        parent_dir = os.path.dirname(current_dir)\n",
    "        if parent_dir == current_dir:\n",
    "            raise FileNotFoundError(f\"Could not find {marker_filename} in any parent directory.\")\n",
    "        current_dir = parent_dir\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--project_root\",  type=str, default=None, help=\"Path to project root (containing .project-root)\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "if args.project_root:\n",
    "    project_root = args.project_root\n",
    "else:\n",
    "    project_root = find_project_root()\n",
    "\n",
    "dotenv_path = os.path.join(project_root, \".env\")\n",
    "load_dotenv(dotenv_path)``\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = VLMTool(api_key=api_key)\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a creative poet.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short poem about a lonely star.\"}\n",
    "]\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=chat_messages,\n",
    "    model=\"gpt-4o\",\n",
    "    max_tokens=150,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44f7d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c5f5816",
   "metadata": {},
   "source": [
    "### 2. Agentic object detection pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603da86",
   "metadata": {},
   "source": [
    "### 3. Running the object detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a976f8",
   "metadata": {},
   "source": [
    "### 4. Why Numbered Batching for Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b413256",
   "metadata": {},
   "source": [
    "### 5. Critiquing and Refining the Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809ae88",
   "metadata": {},
   "source": [
    "### 6. Validation Bounding Box predictions utilizing the VLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
